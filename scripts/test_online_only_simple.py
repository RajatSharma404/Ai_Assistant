"""Simple test for online-only LLM configuration without full initialization"""
import sys
import os

# Don't import from ai_assistant which triggers full initialization
sys.path.insert(0, 'f:/bn/assitant')

# Temporarily skip full module loading
os.environ['SKIP_INIT'] = '1'

try:
    # Direct import without triggering __init__
    import importlib.util
    
    spec = importlib.util.spec_from_file_location(
        "network_aware_llm", 
        "f:/bn/assitant/ai_assistant/modules/network_aware_llm.py"
    )
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    
    print("‚úÖ Module loaded successfully")
    
    # Test the configuration
    config = module.get_optimal_llm_config()
    print(f"‚úÖ Default provider: {config['provider']}")
    print(f"‚úÖ Default model: {config['model']}")
    print(f"‚úÖ Network status: {config['network_status']}")
    print(f"‚úÖ API base: {config.get('api_base', 'default')}")
    
    # Check class exists
    print(f"‚úÖ OnlineLLMConfig class exists: {hasattr(module, 'OnlineLLMConfig')}")
    print(f"‚úÖ NetworkAwareLLMConfig alias exists: {hasattr(module, 'NetworkAwareLLMConfig')}")
    
    # Check no local functions exist
    print(f"‚úÖ force_local_mode removed: {not hasattr(module, 'force_local_mode')}")
    
    print("\nüéâ All tests passed! Online-only mode is working correctly.")
    print("   - Only OpenAI and Gemini providers")
    print("   - No local LLM support")
    print("   - Backward compatibility maintained")
    
except Exception as e:
    print(f"‚ùå Test failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
